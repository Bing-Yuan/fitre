{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demo of the KFAC solver on CIFAR10/100 using several convolutional networks. Require `torch` and `torchvision` to be installed before running. Tested on Python 3.7 with Torch 1.7 as well as Python 3.6 with Torch 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import time\n",
    "from math import floor\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "\n",
    "from tr_kfac_opt import KFACOptimizer\n",
    "from nn_models import *  # contains several common networks\n",
    "from utils import fmt_args, pp_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's first set up the argument parser for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpArgParser(argparse.ArgumentParser):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(description='FITRE experiment -- using KFAC', *args, **kwargs)\n",
    "\n",
    "        self.add_argument('--benchmark', type=str, choices=['cifar10', 'cifar100'], default='cifar10',\n",
    "                          help='the benchmark to run')\n",
    "\n",
    "        self.add_argument('--model', type=str, default='QAlexNetS',\n",
    "                          choices=['QAlexNetS', 'QAlexNetSb', 'VGG16', 'VGG16b'],\n",
    "                          help='neural network model')\n",
    "        self.add_argument('--init', type=str, default='def',\n",
    "                          choices=['def', 'km', 'xavier', '0', '1'],\n",
    "                          help='init network model')\n",
    "\n",
    "        self.add_argument('--batch-size', type=int, default=200,\n",
    "                          help='input batch size for training')\n",
    "        self.add_argument('--test-batch-size', type=int, default=200,\n",
    "                          help='input batch size for testing')\n",
    "        self.add_argument('--epochs', type=int, default=10,\n",
    "                          help='number of epochs to train (default: 10)')\n",
    "        self.add_argument('--da', type=int, default=1)\n",
    "\n",
    "        self.add_argument('--seed', type=int, default=1,\n",
    "                          help='random seed (default: 1)')\n",
    "\n",
    "        # kfac related arguments\n",
    "        self.add_argument('--weight-decay', type=float, default=0, metavar='weight',\n",
    "                          help='learning rate (default: 0)')\n",
    "        self.add_argument('--damp', type=float, default=0.01, metavar='damp',\n",
    "                          help='damping (default: 0.01)')\n",
    "        self.add_argument('--max-delta', type=float, default=100, metavar='maxdelta',\n",
    "                          help='max delta (default: 100)')\n",
    "        self.add_argument('--check-grad', action='store_true', default=False,\n",
    "                          help='gradient')\n",
    "        return\n",
    "\n",
    "    def parse_args(self, args=None, namespace=None):\n",
    "        res = super().parse_args(args, namespace)\n",
    "\n",
    "        # set random seed for all\n",
    "        random.seed(res.seed)\n",
    "        np.random.seed(res.seed)\n",
    "        torch.manual_seed(res.seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(res.seed)\n",
    "        \n",
    "        res.cuda = torch.cuda.is_available()\n",
    "        return res\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's generate networks of specific architectures to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(args: argparse.Namespace) -> nn.Module:\n",
    "    num_classes = {\n",
    "        'cifar10': 10,\n",
    "        'cifar100': 100\n",
    "    }[args.benchmark]\n",
    "\n",
    "    model_list = {\n",
    "        # b means batch normalization\n",
    "        'QAlexNetS': lambda: QAlexNetS(num_classes=num_classes),\n",
    "        'QAlexNetSb': lambda: QAlexNetSb(num_classes=num_classes),\n",
    "        'VGG16': lambda: VGG('VGG16', num_class=num_classes),\n",
    "        'VGG16b': lambda: VGGb('VGG16', num_class=num_classes),\n",
    "    }\n",
    "    model = model_list[args.model]()\n",
    "\n",
    "    if args.init == \"km\":\n",
    "        model.apply(init_params)\n",
    "    elif args.init == \"xavier\":\n",
    "        model.apply(normal_init)\n",
    "    elif args.init == \"0\":\n",
    "        model.apply(zeros_init)\n",
    "    elif args.init == '1':\n",
    "        model.apply(ones_init)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's prepare the datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(args: argparse.Namespace, train: bool) -> Dataset:\n",
    "    if args.benchmark == 'cifar10':\n",
    "        if args.da == 0:\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "            ])\n",
    "        else:\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        transform = transform_train if train else transform_test\n",
    "        return CIFAR10('./data', train=train, download=True, transform=transform)\n",
    "\n",
    "    if args.benchmark == 'cifar100':\n",
    "        if args.da == 0:\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "            ])\n",
    "        else:\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        transform = transform_train if train else transform_test\n",
    "        return CIFAR100('./data', train=train, download=True, transform=transform)\n",
    "\n",
    "    raise NotImplementedError(f'Benchmark {args.benchmark} unsupported yet.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the experiments, using default hyper-parameters of KFAC optimizer. Off notebook, more customized commands of the following shape can be executed:\n",
    "```\n",
    "python3 kfac_exp.py --batch-size 200 --epochs 200 --model QAlexNetS --seed 1 --init def --damp=0.01 --check-grad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== configuration =====\n",
      "  benchmark: cifar10\n",
      "  model: QAlexNetS\n",
      "  init: def\n",
      "  batch_size: 200\n",
      "  test_batch_size: 200\n",
      "  epochs: 10\n",
      "  da: 1\n",
      "  seed: 1\n",
      "  weight_decay: 0\n",
      "  damp: 0.01\n",
      "  max_delta: 100\n",
      "  check_grad: False\n",
      "  cuda: True\n",
      "===== end of configuration =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = ExpArgParser()\n",
    "# args = parser.parse_args()  # uncomment to use this line for command line usage\n",
    "args = parser.parse_args([])  # for notebook demo, we pass in [] to use default arguments\n",
    "print(fmt_args(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Before any training, the test set loss is 2.30317054271698, accuracy is 0.0965.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if args.cuda else torch.device('cpu')\n",
    "model = get_net(args).to(device)\n",
    "\n",
    "loader_kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_ds = get_data_loader(args, True)  # reload training set loader in every epoch\n",
    "test_loader = DataLoader(get_data_loader(args, False),\n",
    "                         batch_size=args.batch_size, shuffle=False, **loader_kwargs)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def eval_test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    correct = 0\n",
    "    total_batches = len(test_loader)\n",
    "    total_pts = len(test_loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            test_loss += criterion(output, target).item() * len(data)\n",
    "            correct += (pred == target).sum().item()\n",
    "            print(f'Evaluated {i}/{total_batches} batches...', end='\\r')\n",
    "\n",
    "    test_loss /= total_pts\n",
    "    acc = 1.0 * correct / total_pts\n",
    "    model.train()\n",
    "    return test_loss, acc\n",
    "\n",
    "loss, acc = eval_test(model)\n",
    "print(f'\\rBefore any training, the test set loss is {loss}, accuracy is {acc}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define KFACOptimizer and run multiple epoches of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1m 34s (94.096 seconds)] Epoch 0 -- Test loss: 1.0403913414478303, Test accuracy: 0.6421.\n",
      "[3m 8s (188.136 seconds)] Epoch 1 -- Test loss: 0.9435971391201019, Test accuracy: 0.6757.\n",
      "[4m 42s (282.982 seconds)] Epoch 2 -- Test loss: 0.8827460837364197, Test accuracy: 0.6969.\n",
      "[6m 17s (377.338 seconds)] Epoch 3 -- Test loss: 0.8288209521770478, Test accuracy: 0.7157.\n",
      "[7m 51s (471.877 seconds)] Epoch 4 -- Test loss: 0.8044604575634002, Test accuracy: 0.7276.\n",
      "[9m 26s (566.796 seconds)] Epoch 5 -- Test loss: 0.8292316210269928, Test accuracy: 0.7239.\n",
      "[11m 1s (661.799 seconds)] Epoch 6 -- Test loss: 0.7942041862010956, Test accuracy: 0.7329.\n",
      "[12m 36s (756.352 seconds)] Epoch 7 -- Test loss: 0.7785956192016602, Test accuracy: 0.7408.\n",
      "[14m 10s (850.842 seconds)] Epoch 8 -- Test loss: 0.7730200099945068, Test accuracy: 0.7438.\n",
      "[15m 45s (945.526 seconds)] Epoch 9 -- Test loss: 0.7478364408016205, Test accuracy: 0.7529.\n"
     ]
    }
   ],
   "source": [
    "kfac_opt = KFACOptimizer(model=model,\n",
    "                         momentum=0.0,\n",
    "                         stat_decay=0.8,\n",
    "                         kl_clip=1e-0,\n",
    "                         damping=args.damp,\n",
    "                         weight_decay=args.weight_decay,\n",
    "                         check_grad=args.check_grad,\n",
    "                         max_delta=args.max_delta,\n",
    "                         Tf=1)\n",
    "t0 = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, **loader_kwargs)\n",
    "    tot_batches = len(train_loader)\n",
    "\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # round 1 backprop\n",
    "        outputs = model(inputs)\n",
    "        kfac_opt.zero_grad()\n",
    "        kfac_opt.acc_stats = True\n",
    "        obj = criterion(outputs, outputs.argmax(dim=1))  # requires classification\n",
    "        obj.backward(retain_graph=True)\n",
    "\n",
    "        # round 2 backprop\n",
    "        kfac_opt.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        kfac_opt.acc_stats = False\n",
    "        loss.backward(create_graph=True)\n",
    "\n",
    "        def _batch_loss():\n",
    "            with torch.no_grad():\n",
    "                # It's only for computing the loss, which needs no grad.\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "                _loss = criterion(outputs, labels).item()\n",
    "            model.train()\n",
    "            return _loss\n",
    "\n",
    "        kfac_opt.step(closure=_batch_loss)\n",
    "        print(f'[{pp_time(time.time() - t0)}] Epoch {epoch}, batch {i} / {tot_batches}, loss {loss.item()}',\n",
    "              end='\\r')\n",
    "\n",
    "    test_loss, test_acc = eval_test(model)\n",
    "    print(f'[{pp_time(time.time() - t0)}] Epoch {epoch} -- Test loss: {test_loss}, Test accuracy: {test_acc}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now training has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}